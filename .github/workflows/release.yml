name: "Release"  
  
on:  
  push:  
    tags:  
      - 'v*'  
  workflow_dispatch:
jobs:  
  publish-tauri:  
    permissions:  
      contents: write  
    strategy:  
      fail-fast: false  
      matrix:  
        include:  
          # Windows - NVIDIA CUDA  
          - platform: 'windows-latest'  
            target: ''  
            asset_name: 'llama-backend-windows-cuda.zip'  
              
          # Linux - CPU/GPU (通用) - 已改为 22.04
          - platform: 'ubuntu-22.04'  
            target: ''  
            asset_name: 'llama-backend-ubuntu.zip'  
              
          # macOS - Intel (x86_64)  
          - platform: 'macos-15'  
            target: 'x86_64-apple-darwin'  
            asset_name: 'llama-backend-macos-intel.zip'  
              
          # macOS - Apple Silicon (ARM64)  
          - platform: 'macos-15'  
            target: 'aarch64-apple-darwin'  
            asset_name: 'llama-backend-macos-applesilicon.zip'  
  
    runs-on: ${{ matrix.platform }}  
    steps:  
      - uses: actions/checkout@v4  
  
      - name: Setup Node.js  
        uses: actions/setup-node@v4  
        with:  
          node-version: 20  
  
      - name: Install Rust stable  
        uses: dtolnay/rust-toolchain@stable  
        with:  
          targets: ${{ matrix.target }}  
  
      # Ubuntu 依赖 (针对 22.04 进行了优化)
      - name: Install dependencies (Ubuntu only)
        if: matrix.platform == 'ubuntu-22.04'
        run: |
          sudo apt-get update
          # 添加了 libgomp1, 这是 llama.cpp 运行必备的 OpenMP 库
          sudo apt-get install -y libwebkit2gtk-4.1-dev libappindicator3-dev librsvg2-dev patchelf libfuse2 libgomp1 file
          
      # 下载各平台对应的 llama.cpp 二进制文件  
      - name: Download llama.cpp binaries  
        shell: bash  
        run: |  
          BACKEND_DIR="src-tauri/resources/llama-backend"  
          mkdir -p "$BACKEND_DIR"  
            
          ASSET_URL="https://github.com/Atom112/AIO/releases/download/llama-binaries/${{ matrix.asset_name }}"  
          echo "Downloading ${{ matrix.asset_name }} for ${{ matrix.platform }}..."  
            
          curl -L -o backend.zip "$ASSET_URL"  
          unzip -o backend.zip -d "$BACKEND_DIR"  # 增加 -o 强制覆盖
          rm backend.zip  
              
          if [[ "${{ matrix.platform }}" != "windows-latest" ]]; then  
            # 暴力赋予整个 backend 文件夹执行权限，防止漏掉某些 cli 工具
            chmod -R +x "$BACKEND_DIR"
            
            # 调试：显示文件类型，确保下载的是 ELF 格式
            echo "Checking file types:"
            file "$BACKEND_DIR"/*
          fi  

      # 验证关键文件是否存在及可用性
      - name: Verify binaries  
        shell: bash  
        run: |  
          if [[ "${{ matrix.platform }}" == "ubuntu-22.04" ]]; then
            # 检查 ldd 是否能解析依赖。如果这一步报错，说明下载的二进制文件与系统不兼容
            echo "Testing ldd on backend binaries..."
            ldd src-tauri/resources/llama-backend/llama-server || echo "Warning: ldd failed on llama-server"
            ldd src-tauri/resources/llama-backend/llama-gemma3-cli || echo "Warning: ldd failed on llama-gemma3-cli"
          fi
          
      - name: Install frontend dependencies  
        run: npm install  
  
      # Tauri 构建和发布  
      - name: Build and Release  
        uses: tauri-apps/tauri-action@v0  
        env:  
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}  
          # 核心修复
          APPIMAGE_EXTRACT_AND_RUN: 1  
          NO_STRIP: 1  
          # 增加这一行，有时能绕过 linuxdeploy 的一些库校验错误
          LD_LIBRARY_PATH: /usr/local/lib
        with:  
          tagName: v__VERSION__  
          releaseName: "AIO v__VERSION__"  
          releaseDraft: true  
          prerelease: false  
          # 确保 verbose 开启以追踪后续错误
          args: ${{ matrix.target != '' && format('--target {0} --verbose', matrix.target) || '--verbose' }}