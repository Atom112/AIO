name: "Release"

on:
  push:
    tags:
      - 'v*'

jobs:
  publish-tauri:
    permissions:
      contents: write
    strategy:
      fail-fast: false
      matrix:
        include:
          # Windows - NVIDIA CUDA
          - platform: 'windows-latest'
            target: ''
            asset_name: 'llama-backend-windows-cuda.zip'
            
          # Linux - 使用 22.04 以确保 AppImage 构建成功
          - platform: 'ubuntu-22.04'
            target: ''
            asset_name: 'llama-backend-ubuntu.zip'
            
          # macOS - Intel (x86_64)
          - platform: 'macos-15'
            target: 'x86_64-apple-darwin'
            asset_name: 'llama-backend-macos-intel.zip'
            
          # macOS - Apple Silicon (ARM64)
          - platform: 'macos-15'
            target: 'aarch64-apple-darwin'
            asset_name: 'llama-backend-macos-applesilicon.zip'

    runs-on: ${{ matrix.platform }}
    steps:
      - uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: 20

      - name: Install Rust stable
        uses: dtolnay/rust-toolchain@stable
        with:
          targets: ${{ matrix.target }}

      - name: Install dependencies (Ubuntu only)
        if: matrix.platform == 'ubuntu-22.04'
        run: |
          sudo apt-get update
          sudo apt-get install -y libwebkit2gtk-4.1-dev libappindicator3-dev librsvg2-dev patchelf
          
      - name: Download llama.cpp binaries
        shell: bash
        run: |
          BACKEND_DIR="src-tauri/resources/llama-backend"
          mkdir -p "$BACKEND_DIR"
          ASSET_URL="https://github.com/Atom112/AIO/releases/download/llama-binaries/${{ matrix.asset_name }}"
          
          echo "Downloading ${{ matrix.asset_name }} for ${{ matrix.platform }}..."
          curl -L -o backend.zip "$ASSET_URL"
          
          # 解压并处理子目录
          TEMP_DIR=$(mktemp -d)
          unzip backend.zip -d "$TEMP_DIR"
          rm backend.zip
          
          # 移动文件（处理子目录情况）
          if [ -d "$TEMP_DIR"/* ]; then
            SUBDIR=$(find "$TEMP_DIR" -mindepth 1 -maxdepth 1 -type d | head -1)
            if [ -n "$SUBDIR" ]; then
              mv "$SUBDIR"/* "$BACKEND_DIR"/
            else
              mv "$TEMP_DIR"/* "$BACKEND_DIR"/
            fi
          fi
          rm -rf "$TEMP_DIR"
          
          # 设置可执行权限（非 Windows）
          if [[ "${{ matrix.platform }}" != "windows-latest" ]]; then
            chmod +x "$BACKEND_DIR"/llama-server* 2>/dev/null || true
            chmod +x "$BACKEND_DIR"/llama-cli* 2>/dev/null || true
          fi
          
          ls -lah "$BACKEND_DIR/"

      - name: Install frontend dependencies
        run: npm install

      - name: Build and Release
        uses: tauri-apps/tauri-action@v0
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        with:
          tagName: v__VERSION__
          releaseName: "AIO v__VERSION__"
          releaseBody: |
            ## 安装包说明
            
            - **Windows**: 内含 CUDA 支持，需要 NVIDIA 显卡和驱动
            - **Linux (Ubuntu 22.04+)**: CPU 模式运行，兼容性好
            - **macOS Intel**: 适用于 Intel 芯片的 Mac
            - **macOS Apple Silicon**: 适用于 M1/M2/M3 芯片的 Mac
          releaseDraft: true
          prerelease: false
          args: ${{ matrix.target != '' && format('--target {0}', matrix.target) || '' }}