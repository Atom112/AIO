name: "Release"

on:
  push:
    tags:
      - 'v*'

jobs:
  publish-tauri:
    permissions:
      contents: write
    strategy:
      fail-fast: false
      matrix:
        include:
          # Windows - NVIDIA CUDA
          - platform: 'windows-latest'
            target: ''
            asset_name: 'llama-backend-windows-cuda.zip'
            
          # Linux - CPU/GPU (通用)
          - platform: 'ubuntu-24.04'
            target: ''
            asset_name: 'llama-backend-ubuntu.zip'
            
          # macOS - Intel (x86_64)
          - platform: 'macos-15'
            target: 'x86_64-apple-darwin'
            asset_name: 'llama-backend-macos-intel.zip'
            
          # macOS - Apple Silicon (ARM64)
          - platform: 'macos-15'
            target: 'aarch64-apple-darwin'
            asset_name: 'llama-backend-macos-applesilicon.zip'

    runs-on: ${{ matrix.platform }}
    steps:
      - uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: 20

      - name: Install Rust stable
        uses: dtolnay/rust-toolchain@stable
        with:
          targets: ${{ matrix.target }}

      # Ubuntu 24.04 依赖
      - name: Install dependencies (Ubuntu only)
        if: matrix.platform == 'ubuntu-24.04'
        run: |
          sudo apt-get update
          sudo apt-get install -y libwebkit2gtk-4.1-dev libappindicator3-dev librsvg2-dev patchelf

      # 下载各平台对应的 llama.cpp 二进制文件
      - name: Download llama.cpp binaries
        shell: bash
        run: |
          BACKEND_DIR="src-tauri/resources/llama-backend"
          mkdir -p "$BACKEND_DIR"
          
          ASSET_URL="https://github.com/Atom112/AIO/releases/download/llama-binaries/${{ matrix.asset_name }}"

          echo "Downloading ${{ matrix.asset_name }} for ${{ matrix.platform }}..."
          echo "URL: $ASSET_URL"
          
          # 下载并解压
          if [[ "${{ matrix.platform }}" == "windows-latest" ]]; then
            curl -L -o backend.zip "$ASSET_URL"
            unzip backend.zip -d "$BACKEND_DIR"
            rm backend.zip
          else
            curl -L -o backend.zip "$ASSET_URL"
            unzip backend.zip -d "$BACKEND_DIR"
            rm backend.zip
            
            # Linux/macOS: 设置可执行权限
            chmod +x "$BACKEND_DIR"/llama-server* 2>/dev/null || true
            chmod +x "$BACKEND_DIR"/llama-cli* 2>/dev/null || true
            
            # 显示下载的文件（调试用）
            echo "Downloaded files:"
            ls -lah "$BACKEND_DIR/"
          fi

      # 验证关键文件是否存在
      - name: Verify binaries
        shell: bash
        run: |
          if [[ "${{ matrix.platform }}" == "windows-latest" ]]; then
            if [ ! -f "src-tauri/resources/llama-backend/llama-server.exe" ]; then
              echo "❌ llama-server.exe not found!"
              exit 1
            fi
            echo "✅ Windows binaries verified"
          else
            if [ ! -f "src-tauri/resources/llama-backend/llama-server" ]; then
              # 有些压缩包可能包含 .exe 后缀即使对于 Linux，检查两种可能
              if [ ! -f "src-tauri/resources/llama-backend/llama-server.exe" ]; then
                echo "❌ llama-server binary not found!"
                ls -la src-tauri/resources/llama-backend/
                exit 1
              fi
            fi
            echo "✅ Unix binaries verified"
          fi

      - name: Install frontend dependencies
        run: npm install

      # Tauri 构建和发布
      - name: Build and Release
        uses: tauri-apps/tauri-action@v0
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        with:
          tagName: v__VERSION__
          releaseName: "AIO v__VERSION__"
          releaseBody: |
            ## 安装包说明
            
            - **Windows**: 内含 CUDA 支持，需要 NVIDIA 显卡和驱动
            - **Linux (Ubuntu 24.04+)**: CPU 模式运行，兼容性好
            - **macOS Intel**: 适用于 Intel 芯片的 Mac
            - **macOS Apple Silicon**: 适用于 M1/M2/M3 芯片的 Mac
            
            首次启动本地模型前，请确保已下载 GGUF 模型文件。
          releaseDraft: true
          prerelease: false
          # macOS 需要指定 target 以构建对应架构
          args: ${{ matrix.target != '' && format('--target {0}', matrix.target) || '' }}